---
title: "Data Science Term Project"
author: "CSORNAI, Gyula - 134706"
date: "March 4, 2017"
output: html_document
classoption: landscape
---



## Requirements

* Choose a publicly available dataset with at least 10,000 records and 10 variables.
* Do EDA, data cleaning
* Try several supervised learning methods (and several values for the parameters for each method).
* Do model selection and evaluation properly (train-test or cross validation etc.) Show various diagnostics (e.g. ROC curve for classification etc.)
* Discuss which algorithms work best on your data and possibly why.
* Bonus: Try the algorithms on the raw data without the cleaning. Are the results the same or worse?

## Submitted modeling plan

* RF 
* GBM
* KNN
* LogReg
* Lasso

## Data and origin

### Data description and relevance

Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0)) 
Prediction task is to determine whether a person makes over 50K a year.

### Data source

**Lichman, M. (2013).**   *UCI Machine Learning Repository* [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

**Donor** 
Ronny Kohavi and Barry Becker 
Data Mining and Visualization 
Silicon Graphics. 

```{r setup, include=FALSE}
## initial cleanup
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)

## libraries
library(pander)
library(ggplot2)

## acquiring data from source
odf<- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", sep=",", header=FALSE)
testdf <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test", sep=",", header=FALSE)

## The source of multiplot function
## http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/ 

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


```

## Exploratory data analysis

* age - The age of the individual
* work_class - The type of employer the individual has
* fnlwgt - The number of people the census takers believe that observation represents
* education - The highest level of education achieved for that individual
* eduY - Highest level of education in numerical form
* mar_stat - Marital status of the individual
* occupation - The occupation of the individual
* relationship -  Contains family relationship values 
* race - descriptions of the individuals race
* sex - Biological Sex in 2 base categories
* cap_gain - Capital gains recorded
* cap_loss - Capital Losses recorded
* hrpw - Hours worked per week
* nat_ctry - Country of origin for person
* mt50K - Whether or not the person makes more than $50,000 per annum income.

### Summary statistics of the data

```{r eda_str, echo=FALSE}

## Feature engineering01 - giving the correct names

fRawHeaderBackfill <- function(f){
  names(f)[names(f)=="V1"] <- 'age'
  names(f)[names(f)=="V2"] <- 'work_class'
  names(f)[names(f)=="V3"] <- 'fnlwgt'
  names(f)[names(f)=="V4"] <- 'education'
  names(f)[names(f)=="V5"] <- 'eduY'
  names(f)[names(f)=="V6"] <- 'mar_stat'
  names(f)[names(f)=="V7"] <- 'occupation'
  names(f)[names(f)=="V8"] <- 'relationship'
  names(f)[names(f)=="V9"] <- 'race'
  names(f)[names(f)=="V10"] <- 'sex'
  names(f)[names(f)=="V11"] <- 'cap_gain'
  names(f)[names(f)=="V12"] <- 'cap_loss'
  names(f)[names(f)=="V13"] <- 'hrpw'
  names(f)[names(f)=="V14"] <- 'nat_ctry'
  names(f)[names(f)=="V15"] <- 'mt50K'
  ##str(f)
  return(f)
}

testdf2 <- subset(testdf, testdf$V1!= "|1x3 Cross validator")
testdf2$V1 <- as.numeric(testdf2$V1)

df <- fRawHeaderBackfill(odf)
tdf <- fRawHeaderBackfill(testdf2)
testdf <- NULL
testdf2<- NULL
odf <- NULL

pander(summary(df))

```

### Data cleaning

* Removing ? values
* Transforming output variable to factor
* Omitting fnlwgt: The number of people the census takers believe that observation represents.

```{r eda_cln, echo=FALSE, include=FALSE}

cdf <- subset(df, df$work_class!=" ?"&
                df$occupation!= " ?" &
                df$nat_ctry != " ?"
                )
ctdf <- subset(tdf, tdf$work_class!=" ?"&
                 tdf$occupation!= " ?" &
                 tdf$nat_ctry != " ?"
)

cdf$mt50K <- as.factor(ifelse(cdf$mt50K == " <=50K",0,1 ))

```

### Data visualization

#### Single distribution of continuous variables

```{r eda_histmpl, echo=FALSE, warning=FALSE, fig.width=10, fig.height=5}

h1 <- ggplot(cdf)+aes(x=age)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Age distribution',
    subtitle='50 bins',
    x='Age groups',
    y='# of citizens'
  )+
  theme_bw()

h2 <- ggplot(cdf)+aes(x=eduY)+
  geom_histogram(bins=20, fill='dodgerblue3')+
  labs(
    title='distribution of years of education',
    subtitle='20 bins',
    x='Years spent in education',
    y='# of citizens'
  )+
  theme_bw()

h3 <- ggplot(cdf)+aes(x=cap_gain)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital gain',
    subtitle='50 bins',
    x='Capital gain groups',
    y='# of citizens'
  )+
  theme_bw()

h3B <- ggplot(cdf)+aes(x=cap_gain)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital log gain',
    subtitle='50 bins',
    x='Capital gain groups of\nnon-zero values - log scale ',
    y='# of citizens'
  )+
  scale_x_log10()+
  theme_bw()

h4 <- ggplot(cdf)+aes(x=cap_loss)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital loss',
    subtitle='50 bins',
    x='Capital loss groups',
    y='# of citizens'
  )+
  theme_bw()

h4B <- ggplot(cdf)+aes(x=cap_loss)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital log loss',
    subtitle='50 bins',
    x='Capital loss groups of\nnon-zero values - log scale',
    y='# of citizens'
  )+
  scale_x_log10()+
  theme_bw()

multiplot(h1, h2, h3, h3B, h4, h4B, cols=3)


```

#### Single distributions of factors

##### Work and occupation

```{r eda_work, echo=FALSE, warning=FALSE, fig.width=10}

fb1 <- ggplot(cdf)+aes(x=work_class,fill=work_class)+
  geom_bar() +
  labs(
    title='Distribution of different work classifications',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))


fb4 <- ggplot(cdf)+aes(x=occupation,fill=occupation)+
  geom_bar() +
  labs(
    title='Distribution of different occupations',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))

multiplot(fb1,  fb4, cols=2)


```

##### Education

```{r eda_edu, echo=FALSE, fig.width=10}

ggplot(cdf)+aes(x=education,fill=education)+
  geom_bar() +
  labs(
    title='Distribution of different levels of education',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))


```

##### Personal data

```{r eda_pers, echo=FALSE, warning=FALSE, fig.width=10, fig.height=8}

fb3 <- ggplot(cdf)+aes(x=mar_stat,fill=mar_stat)+
  geom_bar() +
  labs(
    title='Distribution of different martial status',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb5 <- ggplot(cdf)+aes(x=relationship,fill=relationship)+
  geom_bar() +
  labs(
    title='Distribution of different relationships',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb6 <- ggplot(cdf)+aes(x=race,fill=race)+
  geom_bar() +
  labs(
    title='Distribution of different races',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb7 <- ggplot(cdf)+aes(x=sex,fill=sex)+
  geom_bar() +
  labs(
    title='Distribution of different sexes',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

multiplot(fb3,fb5, fb6, fb7, cols=2)


```

##### Original nationality

```{r eda_nat, echo=FALSE, warning=FALSE, fig.width=10}

fb8 <- ggplot(cdf)+aes(x=nat_ctry,fill=nat_ctry)+
  geom_bar() +
  labs(
    title='Distribution of different native country',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))


fb8B <- ggplot(cdf)+aes(x=nat_ctry,fill=nat_ctry)+
  geom_bar() +
  scale_y_log10()+
  labs(
    title='Distribution of different native country',
    subtitle='Y axis on logarytmic scale',
    y='# of citizens\nlog',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

multiplot(fb8, fb8B, cols=2)


```

#### Wage category - the future prediction outcome

```{r eda_y, echo=FALSE, warning=FALSE, fig.width=10}

ggplot(cdf)+aes(x=mt50K,fill=mt50K)+
  geom_bar()+
  labs(
    title='Distribution of wage categories',
    subtitle='The output vairable of the modeling',
    y='# of citizens',
    x='Is the annual wage more than 50,000 USD?'
  )+
  theme_bw()+
  theme(legend.position="none")

```

### Joint distributions of different variables

#### Discrete variables

```{r eda_jdisc, echo=FALSE, warning=FALSE, fig.width=10, fig.height=12}

jd1<- ggplot(cdf)+aes(x=education, y=occupation)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of levels of education\nand occupation',
    y='Occupation',
    x='level of education'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

jd2<- ggplot(cdf)+aes(x=relationship, y=occupation)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of relationship status\nand occupation',
    y='Occupation',
    x='relationship status'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))
  
jd3 <- ggplot(cdf)+aes(x=education, y=work_class)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of education\nand occupation',
    y='Work class',
    x='level of education'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

jd4 <- ggplot(cdf)+aes(x=mar_stat, y=relationship)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of martial status\nand relationship',
    y='Relationship',
    x='martial status'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

multiplot(jd1, jd2, jd3, jd4, cols=2)


```

#### Continuous variables

##### Dependencies between age and capital by sex


```{r eda_jcdist_sex,echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}

jdc1 <- ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~sex)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by sex',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()

jdc2 <- ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~sex)+
  labs(
    title='Joint distribution of age and capital loss',
    subtitle='Faceted by sex',
    y='Capital loss\non log scale',
    x='Age'
  )+
  theme_bw()

multiplot(jdc1, jdc2, cols=1)


```

##### Dependencies between age and capital gain by work class

```{r eda_jdist_gain_workclass, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~work_class)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by work class',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()

```

##### Dependencies between age and capital loss by work class

```{r eda_jdist_loss_workclass, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~work_class)+
  labs(
    title='Joint distribution of age and capital loss',
    subtitle='Faceted by work class',
    y='Capital loss\non log scale',
    x='age'
  )+
  theme_bw()

```

##### Dependencies between age and capital loss by education

```{r eda_jdist_gain_education, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(900,0),  colours=rainbow(4))+
  facet_wrap(~education)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by education',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()



```

```{r eda_jdist_loss_education, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}

ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(900,0),  colours=rainbow(4))+
  facet_wrap(~education)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by education',
    y='Capital loss\non log scale',
    x='Age'
  )+
  theme_bw()

```

### Further feature engineering

I am dgoint to transform the capital gain and loss values in the following way:

$$ lncg = log(capgain+1) $$
AND

$$ lncl = log(caploss+1) $$

in order to normalize the distribution of the values. This is not a fundamental error, because the majority of the observed citizens have 0 capital gain and 0 capital loss, and whoever has some, has so much, that 1 USD per annum does not really count.

From this point on I will analyze the 2 versions separately, until I can figure it out, whether this transformation has helped or not.

```{r eda_fteng_inc, echo=TRUE}
cdf$fnlwgt <- NULL
cdf$rnd <-runif(nrow(cdf))
cdf <- cdf[order(cdf$rnd),]
cdf$rnd <- NULL
cdfm <- cdf 
cdfm$lncg <- log(cdfm$cap_gain + 1)
cdfm$lncl <- log(cdfm$cap_loss + 1)
cdfm$cap_gain <- NULL
cdfm$cap_loss <- NULL


```


## Supervised learning methods

```{r model_setup,echo=FALSE, warning=FALSE, include=FALSE, error=FALSE}

library(randomForest)
#install.packages('ROCR')
library(ROCR)
#70% training set 30% test set
train_nolog <- cdf[0:round( nrow(cdf) * 0.7 ),]
train_log <- cdfm[0:round( nrow(cdfm) * 0.7 ),]
test_nolog <- cdf[(round( nrow(cdf) * 0.7 )+1) : nrow(cdf),]
test_log <- cdfm[(round( nrow(cdfm) * 0.7 )+1) : nrow(cdfm),]


```

### Analysis with Random forest

#### Modeling with 100 trees

##### Model details

The following chart shows the model details. The confusion matrix is not very descriptive, as it shows accuracy on the training set. 

I acknowledge, that classification error is very low, in case the model predicts lower annual wage, than 50,000 USD, and it is significantly high, in case it tries to predict higher wage, than 50,000 USD. OBB estimate of error rate is relatively low.

```{r mod_rf_t100_smry, echo=FALSE}

#Random forest on no log, 100 trees
rfmod_t100n <- randomForest(mt50K ~ .,data=train_nolog,ntree=100, importance=TRUE)
#Model details
pander(rfmod_t100n)

```

##### Model accuracy

```{r mod_rf_t100_acc, echo=FALSE, warning=FALSE}
#Model validation 
phat <- predict(rfmod_t100n, test_nolog, type = "prob")[,"1"]
#Error rate
error_rate_rf_t100 <- sum(ifelse(phat>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)

```

Confusion matrix below shows, that the actual prediction on the training set is relatively good: the total error rate is `r error_rate_rf_t100*100`%.

```{r mod_rf_t100_confm,echo=FALSE, warning=FALSE}

#Confusion matrix
pander(table(ifelse(phat>0.5,1,0),test_nolog$mt50K))

```

```{r mod_rf_t100_dtl, echo=FALSE, warning=FALSE}

#ROC
rocr_obj <- prediction(phat, test_nolog$mt50K)
#cutoff vs error rate
proc <- performance(rocr_obj, "err")

#AUC
rf_t100_auc <- performance(rocr_obj, "auc")@y.values[[1]]    # AUC
```

The following charts show the details of the modeling with random forest.
The importance of the variable chart (left upper corner) shows, that the most important predictors are 

* relationship 
* capital gain (level!)
* occupation

variables.
The least important are:

* race
* sex
* original country where citizen came from.

The AUC is `r rf_t100_auc`, which is an acceptable model.

```{r mod_rf_t100_viz, echo=FALSE, warning=FALSE, fig.width=10, fig.height=8}

layout(matrix(c(1,2,3,2), 2, 2 , byrow = TRUE),
       widths=c(1,2))
#Importance of variables
varImpPlot(rfmod_t100n, type=2)
#ROC curve
plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE)
plot(proc)

```

#### Modeling multiple random forest model versions

```{r mod_rf_variants, echo=FALSE}

#Random forest on log, 100 trees
rfmod_t100l <- randomForest(mt50K ~ .,data=train_log,ntree=100, importance=TRUE)
phat2 <- predict(rfmod_t100l, test_log, type = "prob")[,"1"]
mod_rf_t100_log_errorrate <- sum(ifelse(phat2>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj2 <- prediction(phat2, test_log$mt50K)
auc2 <- performance(rocr_obj2, "auc")@y.values[[1]]   

#Random forest on level, 500 trees
rfmod_t500n <- randomForest(mt50K ~ .,data=train_nolog,ntree=500, importance=TRUE)
phat3 <- predict(rfmod_t500n, test_nolog, type = "prob")[,"1"]
mod_rf_t500_nolog_errorrate <- sum(ifelse(phat3>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)
rocr_obj3 <- prediction(phat3, test_nolog$mt50K)
auc3 <- performance(rocr_obj3, "auc")@y.values[[1]] 

#Random forest on log, 500 trees
rfmod_t500l <- randomForest(mt50K ~ .,data=train_log,ntree=500, importance=TRUE)
phat4 <- predict(rfmod_t500l, test_log, type = "prob")[,"1"]
mod_rf_t500_log_errorrate <- sum(ifelse(phat4>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj4 <- prediction(phat4, test_log$mt50K)
auc4 <- performance(rocr_obj4, "auc")@y.values[[1]]

#Random forest on level, 500 trees, 5 variables
rfmod_t500n5 <- randomForest(mt50K ~ .,data=train_nolog,ntree=500, mtry=5, importance=TRUE)
phat5 <- predict(rfmod_t500n5, test_nolog, type = "prob")[,"1"]
mod_rf_t500_nolog_var5_errorrate <- sum(ifelse(phat5>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)
rocr_obj5 <- prediction(phat5, test_nolog$mt50K)
auc5 <- performance(rocr_obj5, "auc")@y.values[[1]] 

#Random forest on log, 500 trees, 5 variables
rfmod_t500l5 <- randomForest(mt50K ~ .,data=train_log,ntree=500, mtry=5, importance=TRUE)
phat6 <- predict(rfmod_t500l5, test_log, type = "prob")[,"1"]
mod_rf_t500_log_var5_errorrate <- sum(ifelse(phat6>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj6 <- prediction(phat6, test_log$mt50K)
auc6 <- performance(rocr_obj6, "auc")@y.values[[1]]

```

##### Comparison chart

|Model version / Performance          |Error rate                                 |AUC             |
|:-----------------------------------:|:-----------------------------------------:|:--------------:|
|RF, ntree=100, cap.level, variable=3 |`r error_rate_rf_t100*100`%                |`r rf_t100_auc` |
|RF, ntree=100, cap.log, variable=3   |`r mod_rf_t100_log_errorrate*100`%         |`r auc2`        |
|RF, ntree=500, cap.level, variable=3 |`r mod_rf_t500_nolog_errorrate*100`%       |`r auc3`        |
|RF, ntree=500, cap.log, variable=3   |`r mod_rf_t500_log_errorrate*100`%         |`r auc4`        |
|RF, ntree=500, cap.level, variable=4 |`r mod_rf_t500_nolog_var5_errorrate*100`%  |`r auc5`        |
|RF, ntree=500, cap.log, variable=4   |`r mod_rf_t500_log_var5_errorrate*100`%    |`r auc6`        |


##### Kernel density functions

```{r mod_rf_variants_kd, echo=FALSE, fig.width=10, fig.height=8}

d_phat <- data.frame(phat, mt50K = test_nolog$mt50K)
kdplot1 <- ggplot(d_phat) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t100_v3_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat2 <- data.frame(phat2, mt50K = test_log$mt50K)
kdplot2 <- ggplot(d_phat2) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t100_v3_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat3 <- data.frame(phat3, mt50K = test_nolog$mt50K)
kdplot3 <- ggplot(d_phat3) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v3_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat4 <- data.frame(phat4, mt50K = test_log$mt50K)
kdplot4 <- ggplot(d_phat4) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v3_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat5 <- data.frame(phat5, mt50K = test_nolog$mt50K)
kdplot5 <- ggplot(d_phat5) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v5_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat6 <- data.frame(phat6, mt50K = test_log$mt50K)
kdplot6 <- ggplot(d_phat6) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v5_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

multiplot(kdplot1, kdplot2, kdplot3, kdplot4, kdplot5, kdplot6, cols = 3)

```

##### Importance of variables


```{r mod_rf_variants_imp, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(2,3))
varImpPlot(rfmod_t100n, type=2)
varImpPlot(rfmod_t100l, type=2)
varImpPlot(rfmod_t500n, type=2)
varImpPlot(rfmod_t500l, type=2)
varImpPlot(rfmod_t500n5, type=2)
varImpPlot(rfmod_t500l5, type=2)


```

#### Conclusion for Random Forest

* The Random Forest algorythm gives acceptable quality, roboust results for all parameters.
* It does not really matter whether I use log or level value of capital gain
* In all versions the 2 major performance descriptors (error rate and AUC) were very close for each versions
* There is no visual difference in the kernel density functions
* All versions highlighted the same 3 variables as the most important, and the same 3 as the least important, only the importance was slightly different

### Analysis with GBM

Before modeling it is important to transform the output variable to number, because GBM can use numbers only.

```{r setup_gbm, echo=FALSE, include=FALSE, warning=FALSE}

library(gbm)
train_nolog$mt50K <- ifelse(train_nolog$mt50K==1, 1, 0)
test_nolog$mt50K <- ifelse(test_nolog$mt50K==1, 1, 0) 

```

#### Primary model

The primary model uses the following parameters:

* Number of trees = 100 
* interaction depth = 10
* shrinkage = 0.01 
* cv.folds = 5

##### Confusion matrix

```{r gbm_primary_confmat, echo=FALSE, warning=FALSE}
gbm100t <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli",
          n.trees = 100, interaction.depth = 10, shrinkage = 0.01, cv.folds = 5)

yhat <- predict(gbm100t, test_nolog, n.trees = 100) 

pander(table(ifelse(yhat>0,1,0), test_nolog$mt50K))

```

##### Learning curve

```{r gbm_promary_pcurve, echo=FALSE, warning=FALSE}
gbm.perf(gbm100t, plot.it = TRUE)

```

### Analysis with KNN

### Composite models

## Model selection and validation

### LogReg

### Lasso

## Discussion of effectiveness



