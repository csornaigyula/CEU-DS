---
title: "Data Science Term Project"
author: "CSORNAI, Gyula - 134706"
date: "March 4, 2017"
output: html_document
classoption: landscape
---



## Requirements

* Choose a publicly available dataset with at least 10,000 records and 10 variables.
* Do EDA, data cleaning
* Try several supervised learning methods (and several values for the parameters for each method).
* Do model selection and evaluation properly (train-test or cross validation etc.) Show various diagnostics (e.g. ROC curve for classification etc.)
* Discuss which algorithms work best on your data and possibly why.
* Bonus: Try the algorithms on the raw data without the cleaning. Are the results the same or worse?

## Modeling contains

* RF 
* GBM with grid search
* SVM with grid search


## Data and origin

### Data description and relevance

Extraction was done by Barry Becker from the 1994 Census database. A set of reasonably clean records was extracted using the following conditions: ((AAGE>16) && (AGI>100) && (AFNLWGT>1)&& (HRSWK>0)) 
Prediction task is to determine whether a person makes over 50K a year.

### Data source

**Lichman, M. (2013).**   *UCI Machine Learning Repository* [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

**Donor** 
Ronny Kohavi and Barry Becker 
Data Mining and Visualization 
Silicon Graphics. 

```{r setup, include=FALSE}
## initial cleanup
rm(list=ls())
knitr::opts_chunk$set(echo = TRUE)

## libraries
library(pander)
library(ggplot2)

## acquiring data from source
odf<- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data", sep=",", header=FALSE)
testdf <- read.csv("https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test", sep=",", header=FALSE)

## The source of multiplot function
## http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/ 

multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
  library(grid)
  
  # Make a list from the ... arguments and plotlist
  plots <- c(list(...), plotlist)
  
  numPlots = length(plots)
  
  # If layout is NULL, then use 'cols' to determine layout
  if (is.null(layout)) {
    # Make the panel
    # ncol: Number of columns of plots
    # nrow: Number of rows needed, calculated from # of cols
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                     ncol = cols, nrow = ceiling(numPlots/cols))
  }
  
  if (numPlots==1) {
    print(plots[[1]])
    
  } else {
    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
    
    # Make each plot, in the correct location
    for (i in 1:numPlots) {
      # Get the i,j matrix positions of the regions that contain this subplot
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
      
      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}


```

## Exploratory data analysis

* age - The age of the individual
* work_class - The type of employer the individual has
* fnlwgt - The number of people the census takers believe that observation represents
* education - The highest level of education achieved for that individual
* eduY - Highest level of education in numerical form
* mar_stat - Marital status of the individual
* occupation - The occupation of the individual
* relationship -  Contains family relationship values 
* race - descriptions of the individuals race
* sex - Biological Sex in 2 base categories
* cap_gain - Capital gains recorded
* cap_loss - Capital Losses recorded
* hrpw - Hours worked per week
* nat_ctry - Country of origin for person
* mt50K - Whether or not the person makes more than $50,000 per annum income.

### Summary statistics of the data

```{r eda_str, echo=FALSE}

## Feature engineering01 - giving the correct names

fRawHeaderBackfill <- function(f){
  names(f)[names(f)=="V1"] <- 'age'
  names(f)[names(f)=="V2"] <- 'work_class'
  names(f)[names(f)=="V3"] <- 'fnlwgt'
  names(f)[names(f)=="V4"] <- 'education'
  names(f)[names(f)=="V5"] <- 'eduY'
  names(f)[names(f)=="V6"] <- 'mar_stat'
  names(f)[names(f)=="V7"] <- 'occupation'
  names(f)[names(f)=="V8"] <- 'relationship'
  names(f)[names(f)=="V9"] <- 'race'
  names(f)[names(f)=="V10"] <- 'sex'
  names(f)[names(f)=="V11"] <- 'cap_gain'
  names(f)[names(f)=="V12"] <- 'cap_loss'
  names(f)[names(f)=="V13"] <- 'hrpw'
  names(f)[names(f)=="V14"] <- 'nat_ctry'
  names(f)[names(f)=="V15"] <- 'mt50K'
  ##str(f)
  return(f)
}

testdf2 <- subset(testdf, testdf$V1!= "|1x3 Cross validator")
testdf2$V1 <- as.numeric(testdf2$V1)

df <- fRawHeaderBackfill(odf)
tdf <- fRawHeaderBackfill(testdf2)
testdf <- NULL
testdf2<- NULL
odf <- NULL

pander(summary(df))

```

### Data cleaning

* Removing ? values
* Transforming output variable to factor
* Omitting fnlwgt: The number of people the census takers believe that observation represents.

```{r eda_cln, echo=FALSE, include=FALSE}

cdf <- subset(df, df$work_class!=" ?"&
                df$occupation!= " ?" &
                df$nat_ctry != " ?"
                )
ctdf <- subset(tdf, tdf$work_class!=" ?"&
                 tdf$occupation!= " ?" &
                 tdf$nat_ctry != " ?"
)

cdf$mt50K <- as.factor(ifelse(cdf$mt50K == " <=50K",0,1 ))

```

### Data visualization

#### Single distribution of continuous variables

```{r eda_histmpl, echo=FALSE, warning=FALSE, fig.width=10, fig.height=5}

h1 <- ggplot(cdf)+aes(x=age)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Age distribution',
    subtitle='50 bins',
    x='Age groups',
    y='# of citizens'
  )+
  theme_bw()

h2 <- ggplot(cdf)+aes(x=eduY)+
  geom_histogram(bins=20, fill='dodgerblue3')+
  labs(
    title='distribution of years of education',
    subtitle='20 bins',
    x='Years spent in education',
    y='# of citizens'
  )+
  theme_bw()

h3 <- ggplot(cdf)+aes(x=cap_gain)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital gain',
    subtitle='50 bins',
    x='Capital gain groups',
    y='# of citizens'
  )+
  theme_bw()

h3B <- ggplot(cdf)+aes(x=cap_gain)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital log gain',
    subtitle='50 bins',
    x='Capital gain groups of\nnon-zero values - log scale ',
    y='# of citizens'
  )+
  scale_x_log10()+
  theme_bw()

h4 <- ggplot(cdf)+aes(x=cap_loss)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital loss',
    subtitle='50 bins',
    x='Capital loss groups',
    y='# of citizens'
  )+
  theme_bw()

h4B <- ggplot(cdf)+aes(x=cap_loss)+
  geom_histogram(bins=50, fill='dodgerblue3')+
  labs(
    title='Distribution of capital log loss',
    subtitle='50 bins',
    x='Capital loss groups of\nnon-zero values - log scale',
    y='# of citizens'
  )+
  scale_x_log10()+
  theme_bw()

multiplot(h1, h2, h3, h3B, h4, h4B, cols=3)


```

#### Single distributions of factors

##### Work and occupation

I expect work class to be a bad predictor (many categories, and most of the observation are in one category), occupation may be a relativel good one, because it does not have too many categories, and they are relatively homogenously populated.

```{r eda_work, echo=FALSE, warning=FALSE, fig.width=10}

fb1 <- ggplot(cdf)+aes(x=work_class,fill=work_class)+
  geom_bar() +
  labs(
    title='Distribution of different work classifications',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))


fb4 <- ggplot(cdf)+aes(x=occupation,fill=occupation)+
  geom_bar() +
  labs(
    title='Distribution of different occupations',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))

multiplot(fb1,  fb4, cols=2)


```

##### Education

Education is expected to be a medium-low predictor, because many education categories are not populated well, all are in 3 major categories. Corner cases may not be easily predicted.

```{r eda_edu, echo=FALSE, fig.width=10}

ggplot(cdf)+aes(x=education,fill=education)+
  geom_bar() +
  labs(
    title='Distribution of different levels of education',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-60, hjust=0, size=8))


```

##### Personal data

From the following charts we can learn the following:

* Marital status may be a good candidate of strong predictor variable, because it has 2 major categories and a 3rd strong one. 
* Race has a potential of not being a good predictor, because nealry all observations fall into the same category.
* Relationships from the other side may be a strong predictor, because the distribution is less skewed, we habe multiple well populated categories
* Sex may or may not be strong predictor, we have 2 categories only, and the dataset is unbalanced, we have twice as much males than females

```{r eda_pers, echo=FALSE, warning=FALSE, fig.width=10, fig.height=8}

fb3 <- ggplot(cdf)+aes(x=mar_stat,fill=mar_stat)+
  geom_bar() +
  labs(
    title='Distribution of different martial status',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb5 <- ggplot(cdf)+aes(x=relationship,fill=relationship)+
  geom_bar() +
  labs(
    title='Distribution of different relationships',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb6 <- ggplot(cdf)+aes(x=race,fill=race)+
  geom_bar() +
  labs(
    title='Distribution of different races',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

fb7 <- ggplot(cdf)+aes(x=sex,fill=sex)+
  geom_bar() +
  labs(
    title='Distribution of different sexes',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-45, hjust=0, size=8))

multiplot(fb3,fb5, fb6, fb7, cols=2)


```

##### Original nationality

The following charts show, that nearly all observations fall into the same category, the majority of observations point to US nationality. This will not be a very strong predictor feature, because this feature is true for nearly all observations.

```{r eda_nat, echo=FALSE, warning=FALSE, fig.width=10}

fb8 <- ggplot(cdf)+aes(x=nat_ctry,fill=nat_ctry)+
  geom_bar() +
  labs(
    title='Distribution of different native country',
    y='# of citizens',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))


fb8B <- ggplot(cdf)+aes(x=nat_ctry,fill=nat_ctry)+
  geom_bar() +
  scale_y_log10()+
  labs(
    title='Distribution of different native country',
    subtitle='Y axis on logarytmic scale',
    y='# of citizens\nlog',
    x=''
  )+
  theme_bw()+
  theme(legend.position="none")+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

multiplot(fb8, fb8B, cols=2)


```

#### Wage category - the future prediction outcome

The dataset is not balanced, we have nearly 3 times as many observations under 50,000 USD compared to the ones over 50,000USD. This means, that we cannot simply rely on error rate, because predicting only under 50,000 will produce 25% error rate.

```{r eda_y, echo=FALSE, warning=FALSE, fig.width=10}

ggplot(cdf)+aes(x=mt50K,fill=mt50K)+
  geom_bar()+
  labs(
    title='Distribution of wage categories',
    subtitle='The output vairable of the modeling',
    y='# of citizens',
    x='Is the annual wage more than 50,000 USD?'
  )+
  theme_bw()+
  theme(legend.position="none")

```

### Joint distributions of different variables

#### Discrete variables

When looking on joint distributions of discrete variables the following statements can be made:

* surprisingly marital status seems to be a good segmenter because it has a quite inhomogenous distribution
* occupation looks to be segmenting relatively poorly
* level of education segments better
* relationship segments better too
* work class seems to be on the lower end

This means, that whichever segments poorly will have a higher probability to predict badly in those tree models, where we are subsampling the variables.

```{r eda_jdisc, echo=FALSE, warning=FALSE, fig.width=10, fig.height=12}

jd1<- ggplot(cdf)+aes(x=education, y=occupation)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of levels of education\nand occupation',
    y='Occupation',
    x='level of education'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

jd2<- ggplot(cdf)+aes(x=relationship, y=occupation)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of relationship status\nand occupation',
    y='Occupation',
    x='relationship status'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))
  
jd3 <- ggplot(cdf)+aes(x=education, y=work_class)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of education\nand occupation',
    y='Work class',
    x='level of education'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

jd4 <- ggplot(cdf)+aes(x=mar_stat, y=relationship)+
  geom_count(col='dodgerblue3')+
  labs(
    title='Joint distribution of martial status\nand relationship',
    y='Relationship',
    x='martial status'
  )+
  theme_bw()+
  theme(axis.text.x=element_text(angle=-90, hjust=0, size=8))

multiplot(jd1, jd2, jd3, jd4, cols=2)


```

#### Continuous variables

##### Dependencies between age and capital by sex

The faceted charts below show, that if we try to differentiate between groups based on sex, both on capital gain and capital loss side the pattern will be relatively similar. This assumes that ses will not be a good predictor.

```{r eda_jcdist_sex,echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}

jdc1 <- ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~sex)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by sex',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()

jdc2 <- ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~sex)+
  labs(
    title='Joint distribution of age and capital loss',
    subtitle='Faceted by sex',
    y='Capital loss\non log scale',
    x='Age'
  )+
  theme_bw()

multiplot(jdc1, jdc2, cols=1)


```

##### Dependencies between age and capital gain by work class

The joint distribution between age, capital gain and working class shows, that capital gain and working class produce somewhat different  patterns, still the real differentiator is age. As shown above this is because the inbalance between the working class distributions - the majority of observations fall into private sector.


```{r eda_jdist_gain_workclass, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~work_class)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by work class',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()

```

##### Dependencies between age and capital loss by work class

The joint distribution between age, capital loss and working class shows, that capital loss and working class produce relatively similar patterns and the real differentiator is age.

This means, that only age will be able to segment well in tree based algorythms, and in those cases where these 3 come together in the parameter selection, we will get poor prediction models.

```{r eda_jdist_loss_workclass, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(1600,0),  colours=rainbow(4))+
  facet_wrap(~work_class)+
  labs(
    title='Joint distribution of age and capital loss',
    subtitle='Faceted by work class',
    y='Capital loss\non log scale',
    x='age'
  )+
  theme_bw()

```

##### Dependencies between age and capital gain by education

The following faceted charts plots the joint distribution between age, capital loss and education

The  facet shows, that we can expect education to be a medium-good predictor together with capital gain, because from one side they segment the population to significantly different patterns, however the majority of the population shown on the heatmap with non-deep-blue colors are in 3 categories only:

* HS-Grad
* Some college
* Bachelors

Also we cannot expect significant capital gains in general in some low population degrees.

Age is going to be a good segmenter, which will be important in tree-based methods.

```{r eda_jdist_gain_education, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}
ggplot(cdf)+aes(x=age, y=log(cap_gain+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(900,0),  colours=rainbow(4))+
  facet_wrap(~education)+
  labs(
    title='Joint distribution of age and capital gain',
    subtitle='Faceted by education',
    y='Capital gain\non log scale',
    x='Age'
  )+
  theme_bw()



```

##### Dependencies between age and capital loss by education

The following faceted charts plots the joint distribution between age, capital loss and education

The  facet shows, that we can expect education to be a medium-good predictor together with capital loss, because from one side they segment the population to somewhat different patterns, however the majority of the population shown on the heatmap with non-deep-blue colors are in 3 categories only:

* HS-Grad
* Some college
* Bachelors

From the other age here is still going to be a good segmenter, which will be important in tree-based methods.

```{r eda_jdist_loss_education, echo=FALSE, warning=FALSE, fig.width=10, fig.height=9}

ggplot(cdf)+aes(x=age, y=log(cap_loss+1))+
  geom_bin2d()+
  scale_fill_gradientn(limits=c(900,0),  colours=rainbow(4))+
  facet_wrap(~education)+
  labs(
    title='Joint distribution of age and capital loss',
    subtitle='Faceted by education',
    y='Capital loss\non log scale',
    x='Age'
  )+
  theme_bw()

```

### Further feature engineering

I am dgoint to transform the capital gain and loss values in the following way:

$$ lncg = log(capgain+1) $$
AND

$$ lncl = log(caploss+1) $$

in order to normalize the distribution of the values. This is not a fundamental error, because the majority of the observed citizens have 0 capital gain and 0 capital loss, and whoever has some, has so much, that 1 USD per annum does not really count.

From this point on I will analyze the 2 versions separately, until I can figure it out, whether this transformation has helped or not.

```{r eda_fteng_inc, echo=TRUE}
cdf$fnlwgt <- NULL
cdf$rnd <-runif(nrow(cdf))
cdf <- cdf[order(cdf$rnd),]
cdf$rnd <- NULL
cdfm <- cdf 
cdfm$lncg <- log(cdfm$cap_gain + 1)
cdfm$lncl <- log(cdfm$cap_loss + 1)
cdfm$cap_gain <- NULL
cdfm$cap_loss <- NULL


```


## Supervised learning methods

```{r model_setup,echo=FALSE, warning=FALSE, include=FALSE, error=FALSE}

library(randomForest)
#install.packages('ROCR')
library(ROCR)
#70% training set 30% test set
train_nolog <- cdf[0:round( nrow(cdf) * 0.7 ),]
train_log <- cdfm[0:round( nrow(cdfm) * 0.7 ),]
test_nolog <- cdf[(round( nrow(cdf) * 0.7 )+1) : nrow(cdf),]
test_log <- cdfm[(round( nrow(cdfm) * 0.7 )+1) : nrow(cdfm),]


```

### Analysis with Random forest

#### Modeling with 100 trees

##### Model details

The following chart shows the model details. The confusion matrix is not very descriptive, as it shows accuracy on the training set. 

I acknowledge, that classification error is very low, in case the model predicts lower annual wage, than 50,000 USD, and it is significantly high, in case it tries to predict higher wage, than 50,000 USD. OBB estimate of error rate is relatively low.

```{r mod_rf_t100_smry, echo=FALSE}

#Random forest on no log, 100 trees
rfmod_t100n <- randomForest(mt50K ~ .,data=train_nolog,ntree=100, importance=TRUE)
#Model details
pander(rfmod_t100n)

```

##### Model accuracy

```{r mod_rf_t100_acc, echo=FALSE, warning=FALSE}
#Model validation 
phat <- predict(rfmod_t100n, test_nolog, type = "prob")[,"1"]
#Error rate
error_rate_rf_t100 <- sum(ifelse(phat>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)

```

Confusion matrix below shows, that the actual prediction on the training set is relatively good: the total error rate is `r error_rate_rf_t100*100`%.

```{r mod_rf_t100_confm,echo=FALSE, warning=FALSE}

#Confusion matrix
pander(table(ifelse(phat>0.5,1,0),test_nolog$mt50K))

```

```{r mod_rf_t100_dtl, echo=FALSE, warning=FALSE}

#ROC
rocr_obj <- prediction(phat, test_nolog$mt50K)
#cutoff vs error rate
proc <- performance(rocr_obj, "err")

#AUC
rf_t100_auc <- performance(rocr_obj, "auc")@y.values[[1]]    # AUC
```

The following charts show the details of the modeling with random forest.
The importance of the variable chart (left upper corner) shows, that the most important predictors are 

* relationship 
* capital gain (level!)
* occupation

variables.
The least important are:

* race
* sex
* original country where citizen came from.

The AUC is `r rf_t100_auc`, which is an acceptable model.

```{r mod_rf_t100_viz, echo=FALSE, warning=FALSE, fig.width=10, fig.height=8}

layout(matrix(c(1,2,3,2), 2, 2 , byrow = TRUE),
       widths=c(1,2))
#Importance of variables
varImpPlot(rfmod_t100n, type=2)
#ROC curve
plot(performance(rocr_obj, "tpr", "fpr"), colorize=TRUE)
plot(proc)

```

#### Modeling multiple random forest model versions

```{r mod_rf_variants1, echo=FALSE}

#Random forest on log, 100 trees
rfmod_t100l <- randomForest(mt50K ~ .,data=train_log,ntree=100, importance=TRUE)
phat2 <- predict(rfmod_t100l, test_log, type = "prob")[,"1"]
mod_rf_t100_log_errorrate <- sum(ifelse(phat2>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj2 <- prediction(phat2, test_log$mt50K)
auc2 <- performance(rocr_obj2, "auc")@y.values[[1]]   

```

```{r mod_rf_variants2, echo=FALSE}
#Random forest on level, 500 trees
rfmod_t500n <- randomForest(mt50K ~ .,data=train_nolog,ntree=500, importance=TRUE)
phat3 <- predict(rfmod_t500n, test_nolog, type = "prob")[,"1"]
mod_rf_t500_nolog_errorrate <- sum(ifelse(phat3>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)
rocr_obj3 <- prediction(phat3, test_nolog$mt50K)
auc3 <- performance(rocr_obj3, "auc")@y.values[[1]] 
```

```{r mod_rf_variants3, echo=FALSE}
#Random forest on log, 500 trees
rfmod_t500l <- randomForest(mt50K ~ .,data=train_log,ntree=500, importance=TRUE)
phat4 <- predict(rfmod_t500l, test_log, type = "prob")[,"1"]
mod_rf_t500_log_errorrate <- sum(ifelse(phat4>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj4 <- prediction(phat4, test_log$mt50K)
auc4 <- performance(rocr_obj4, "auc")@y.values[[1]]
```

```{r mod_rf_variants3Btable, echo=FALSE}

rft3<-table(ifelse(phat4>0.5,1,0), test_nolog$mt50K)
```

```{r mod_rf_variants4, echo=FALSE}
#Random forest on level, 500 trees, 5 variables
rfmod_t500n5 <- randomForest(mt50K ~ .,data=train_nolog,ntree=500, mtry=5, importance=TRUE)
phat5 <- predict(rfmod_t500n5, test_nolog, type = "prob")[,"1"]
mod_rf_t500_nolog_var5_errorrate <- sum(ifelse(phat5>0.5,1,0)!=test_nolog$mt50K)/nrow(test_nolog)
rocr_obj5 <- prediction(phat5, test_nolog$mt50K)
auc5 <- performance(rocr_obj5, "auc")@y.values[[1]] 
```

```{r mod_rf_variants5, echo=FALSE}
#Random forest on log, 500 trees, 5 variables
rfmod_t500l5 <- randomForest(mt50K ~ .,data=train_log,ntree=500, mtry=5, importance=TRUE)
phat6 <- predict(rfmod_t500l5, test_log, type = "prob")[,"1"]
mod_rf_t500_log_var5_errorrate <- sum(ifelse(phat6>0.5,1,0)!=test_log$mt50K)/nrow(test_log)
rocr_obj6 <- prediction(phat6, test_log$mt50K)
auc6 <- performance(rocr_obj6, "auc")@y.values[[1]]

```

##### Comparison chart

|Model version / Performance          |Error rate                                 |AUC             |
|:-----------------------------------:|:-----------------------------------------:|:--------------:|
|RF, ntree=100, cap.level, variable=3 |`r error_rate_rf_t100*100`%                |`r rf_t100_auc` |
|RF, ntree=100, cap.log, variable=3   |`r mod_rf_t100_log_errorrate*100`%         |`r auc2`        |
|RF, ntree=500, cap.level, variable=3 |`r mod_rf_t500_nolog_errorrate*100`%       |`r auc3`        |
|RF, ntree=500, cap.log, variable=3   |`r mod_rf_t500_log_errorrate*100`%         |`r auc4`        |
|RF, ntree=500, cap.level, variable=5 |`r mod_rf_t500_nolog_var5_errorrate*100`%  |`r auc5`        |
|RF, ntree=500, cap.log, variable=5   |`r mod_rf_t500_log_var5_errorrate*100`%    |`r auc6`        |


##### Kernel density functions

The kernel density functions show the probability density of the 2 predictions based on scores. Visually there is no significant difference on the following 6 analyzed charts, it shows, that it will give 0 score with a very high probability density (it will identify negative examples well), however positive examples it will classify with a much higher error, there will be quite numerous with low score even if the predicted variable is 12. We need to expect a lot of false negatives (the numbers in the model versions shows the same impact).

```{r mod_rf_variants_kd, echo=FALSE, fig.width=10, fig.height=8}

d_phat <- data.frame(phat, mt50K = test_nolog$mt50K)
kdplot1 <- ggplot(d_phat) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t100_v3_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat2 <- data.frame(phat2, mt50K = test_log$mt50K)
kdplot2 <- ggplot(d_phat2) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t100_v3_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat3 <- data.frame(phat3, mt50K = test_nolog$mt50K)
kdplot3 <- ggplot(d_phat3) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v3_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat4 <- data.frame(phat4, mt50K = test_log$mt50K)
kdplot4 <- ggplot(d_phat4) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v3_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat5 <- data.frame(phat5, mt50K = test_nolog$mt50K)
kdplot5 <- ggplot(d_phat5) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v5_level',
    x='Score',
    y='Density'
  )+
  theme_bw()

d_phat6 <- data.frame(phat6, mt50K = test_log$mt50K)
kdplot6 <- ggplot(d_phat6) +
  geom_density( aes(x = phat, fill = mt50K, col=mt50K), alpha=0.4)+
  labs(
    title='Kernel density - RF_t500_v5_log',
    x='Score',
    y='Density'
  )+
  theme_bw()

multiplot(kdplot1, kdplot2, kdplot3, kdplot4, kdplot5, kdplot6, cols = 3)

```

##### Importance of variables

After the first supervised analysis we can conclude, that relationship, capital gain (both level and log) and age are the most important predictors in all models, while native country, sex and race are not. This was somewhat expected from the distribution charts, however I expected occupation being in the top 3. There is also a surprising difference between the role of capital gain and capital loss, which was somewhat expected but not in this extent.


```{r mod_rf_variants_imp, echo=FALSE, fig.width=10, fig.height=8}
par(mfrow=c(2,3))
varImpPlot(rfmod_t100n, type=2)
varImpPlot(rfmod_t100l, type=2)
varImpPlot(rfmod_t500n, type=2)
varImpPlot(rfmod_t500l, type=2)
varImpPlot(rfmod_t500n5, type=2)
varImpPlot(rfmod_t500l5, type=2)


```

#### Conclusion for Random Forest

* The Random Forest algorythm gives acceptable quality, roboust results for all parameters.
* It does not really matter whether I use log or level value of capital gain
* In all versions the 2 major performance descriptors (error rate and AUC) were very close for each versions
* There is no visual difference in the kernel density functions
* All versions highlighted the same 3 variables as the most important, and the same 3 as the least important, only the importance was slightly different

### Analysis with GBM

Before modeling it is important to transform the output variable to number, because GBM can use numbers only.

GBM is the most sensitive model, when it comes to overfitting, but also promises the best performance in case used correctly.

Because of this there are 2 parameters I will primarily focus on:

* Interaction depth: the dept of the trees 
* shrinkage: regularization parameter to control the gradient descent on error function surface.

```{r setup_gbm, echo=FALSE, include=FALSE, warning=FALSE}

library(gbm)
set.seed(123)
train_nolog$mt50K <- ifelse(train_nolog$mt50K==1, 1, 0)
test_nolog$mt50K <- ifelse(test_nolog$mt50K==1, 1, 0) 

```

#### Primary model

The primary model uses the following parameters:

* Number of trees = 100 
* interaction depth = 10 - The depth of each tree
* shrinkage = 0.01 - Step size reduction in learning while descending on the error rate surface. 1 is full step. Typical shrinkage rates recommended are 0.01 to 0.001
* cv.folds = 5 - Uses 5 fold cross validation
* distribution = *bernoulli* - the loss function

Not yet used

* bag.fraction - subsampling rate

##### Confusion matrix

```{r gbm_primary_confmat, echo=FALSE, warning=FALSE}
gbm_p1 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli",
          n.trees = 100, interaction.depth = 10, shrinkage = 0.01, cv.folds = 5)

yhat <- predict(gbm_p1, test_nolog, n.trees = 100) 

pander(table(ifelse(yhat>0,1,0), test_nolog$mt50K))

```

##### Learning curve

```{r gbm_promary_pcurve, echo=FALSE, warning=FALSE, message=FALSE}
gbm.perf(gbm_p1, plot.it = TRUE)

```

#### Grid search with different parameters

```{r gbm_grid_srch_tbl1, echo=FALSE}

t1 <- table(ifelse(yhat>0,1,0), test_nolog$mt50K)
rocr_obj_gbm1 <- prediction(yhat, test_nolog$mt50K)
auc_gbm1 <- performance(rocr_obj_gbm1, "auc")@y.values[[1]]

```

```{r gbm_grid_srch_tbl2, echo=FALSE}
gbm_500_10_001_5 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, interaction.depth = 10, shrinkage = 0.01, cv.folds = 5)
yhat_gbm2 <- predict(gbm_500_10_001_5, test_nolog, n.trees = 500) 
t2<- table(ifelse(yhat_gbm2>0,1,0), test_nolog$mt50K)
rocr_obj_gbm2 <- prediction(yhat_gbm2, test_nolog$mt50K)
auc_gbm2 <- performance(rocr_obj_gbm2, "auc")@y.values[[1]]

```

```{r gbm_grid_srch_tbl3, echo=FALSE}

gbm_100_20_001_5 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 100, interaction.depth = 20, shrinkage = 0.01, cv.folds = 5)
yhat_gbm3 <- predict(gbm_100_20_001_5, test_nolog, n.trees = 100) 
t3<- table(ifelse(yhat_gbm3>0,1,0), test_nolog$mt50K)
rocr_obj_gbm3 <- prediction(yhat_gbm3, test_nolog$mt50K)
auc_gbm3 <- performance(rocr_obj_gbm3, "auc")@y.values[[1]]

```

```{r gbm_grid_srch_tbl4, echo=FALSE}
gbm_500_20_001_5 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, interaction.depth = 20, shrinkage = 0.01, cv.folds = 5)
yhat_gbm4 <- predict(gbm_500_20_001_5, test_nolog, n.trees = 500) 
t4<- table(ifelse(yhat_gbm4>0,1,0), test_nolog$mt50K)
rocr_obj_gbm4 <- prediction(yhat_gbm4, test_nolog$mt50K)
auc_gbm4 <- performance(rocr_obj_gbm4, "auc")@y.values[[1]]

```

```{r gbm_grid_srch_tbl6, echo=FALSE}
gbm_500_10_0001_5 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, interaction.depth = 10, shrinkage = 0.001, cv.folds = 5)
yhat_gbm6 <- predict(gbm_500_10_0001_5, test_nolog, n.trees = 500) 
t6<- table(ifelse(yhat_gbm6>0,1,0), test_nolog$mt50K)
rocr_obj_gbm6 <- prediction(yhat_gbm6, test_nolog$mt50K)
auc_gbm6 <- performance(rocr_obj_gbm6, "auc")@y.values[[1]]

```

```{r gbm_grid_srch_tbl8, echo=FALSE}

gbm_500_20_0001_5 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, interaction.depth = 20, shrinkage = 0.001, cv.folds = 5)
yhat_gbm8 <- predict(gbm_500_20_0001_5, test_nolog, n.trees = 500) 
t8<- table(ifelse(yhat_gbm8>0,1,0), test_nolog$mt50K)
rocr_obj_gbm8 <- prediction(yhat_gbm8, test_nolog$mt50K)
auc_gbm8 <- performance(rocr_obj_gbm8, "auc")@y.values[[1]]

```

##### Grid search table

|ModelID|Number of trees|Depth of each tree|Shrinkage|CV folds|Result TN  |Result FN  |Result FP  |Result TP  |AUC                                 |
|:-----:|:-------------:|:----------------:|:-------:|:------:|:---------:|:---------:|:---------:|:---------:|:----------------------------------:|
|gbm_p1 |100            |10                |0.01     |5       |`r t1[1,1]`|`r t1[1,2]`|`r t1[2,1]`|`r t1[2,2]`|`r auc_gbm1`|
|gbm_500_10_001_5|500   |10                |0.01     |5       |`r t2[1,1]`|`r t2[1,2]`|`r t2[2,1]`|`r t2[2,2]`|`r auc_gbm2`|
|gbm_100_20_001_5|100   |20                |0.01     |5       |`r t3[1,1]`|`r t3[1,2]`|`r t3[2,1]`|`r t3[2,2]`|`r auc_gbm3`|
|gbm_500_20_001_5|500   |20                |0.01     |5       |`r t4[1,1]`|`r t4[1,2]`|`r t4[2,1]`|`r t4[2,2]`|`r auc_gbm4`|
|gbm_100_10_0001_5|100  |10                |0.001    |5       |6763       |2286       |NA         |NA         |No positive results                |
|gbm_500_10_0001_5|500  |10                |0.001    |5       |`r t6[1,1]`|`r t6[1,2]`|`r t6[2,1]`|`r t6[2,2]`|`r auc_gbm6`|
|gbm_100_20_0001_5|100  |20                |0.001    |5       |6763       |2286       |NA         |NA         |No positive results                |
|gbm_500_20_0001_5|500  |20                |0.001    |5       |`r t8[1,1]`|`r t8[1,2]`|`r t8[2,1]`|`r t8[2,2]`|`r auc_gbm8`|

##### Changing cross validation parameters

```{r gbm_crossval_eff01, echo=FALSE}
gbm_500_20_001_1 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, 
                        interaction.depth = 20, shrinkage = 0.01, cv.folds = 1)
yhat_gbm11 <- predict(gbm_500_20_001_1, test_nolog, n.trees = 500) 
t11<- table(ifelse(yhat_gbm11>0,1,0), test_nolog$mt50K)
rocr_obj_gbm11 <- prediction(yhat_gbm11, test_nolog$mt50K)
auc_gbm11 <- performance(rocr_obj_gbm11, "auc")@y.values[[1]]

```


```{r gbm_crossval_eff02, echo=FALSE}
gbm_500_20_001_10 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, 
                        interaction.depth = 20, shrinkage = 0.01, cv.folds = 10)
yhat_gbm12 <- predict(gbm_500_20_001_10, test_nolog, n.trees = 500) 
t12<- table(ifelse(yhat_gbm12>0,1,0), test_nolog$mt50K)
rocr_obj_gbm12 <- prediction(yhat_gbm12, test_nolog$mt50K)
auc_gbm12 <- performance(rocr_obj_gbm12, "auc")@y.values[[1]]
```

```{r gbm_crossval_eff03, echo=FALSE}
gbm_500_20_001_3 <- gbm(mt50K ~ . ,data=train_nolog, distribution = "bernoulli", n.trees = 500, 
                         interaction.depth = 20, shrinkage = 0.01, cv.folds = 3)
yhat_gbm13 <- predict(gbm_500_20_001_3, test_nolog, n.trees = 500) 
t13<- table(ifelse(yhat_gbm13>0,1,0), test_nolog$mt50K)
rocr_obj_gbm13 <- prediction(yhat_gbm13, test_nolog$mt50K)
auc_gbm13 <- performance(rocr_obj_gbm13, "auc")@y.values[[1]]

```

The following chart shows the impact of changing the folds of cross validation for the best model.


|ModelID        |Number of trees|Depth of each tree|Shrinkage|CV folds|Result TN  |Result FN  |Result FP  |Result TP  |AUC         |
|:-------------:|:-------------:|:----------------:|:-------:|:------:|:---------:|:---------:|:---------:|:---------:|:----------:|
|gbm_500_20_001_5|500           |20                |0.01     |5       |`r t4[1,1]`|`r t4[1,2]`|`r t4[2,1]`|`r t4[2,2]`|`r auc_gbm4`|
|gbm_500_20_001_1|500           |20                |0.01     |1 |`r t11[1,1]`|`r t11[1,2]`|`r t11[2,1]`|`r t11[2,2]`|`r auc_gbm11`|
|gbm_500_20_001_10|500          |20                |0.01     |10|`r t12[1,1]`|`r t12[1,2]`|`r t12[2,1]`|`r t12[2,2]`|`r auc_gbm12`|
|gbm_500_20_001_3|500           |20                |0.01     |3 |`r t13[1,1]`|`r t13[1,2]`|`r t13[2,1]`|`r t13[2,2]`|`r auc_gbm13`|

### Analysis with SVM

For support vector modeling classification the main direction was trying to fing appropriate gamma and cost parameters. SVM uses a certain step function (sigmoid, exponential) for prediction.
Here gamma is the input parameter for non-linear kernel with 1/(data dimension) default value and cost is a regularization parameter with default value 1.

For all models above I used radial kernel.

My expectation fr SVM were robustness, but somewhat lower accuracy.

```{r svm_setup, echo=FALSE,  warning=FALSE}
library(e1071)
set.seed(123)

svm_base <- svm(mt50K ~ ., data = train_nolog,
          kernel = "radial", gamma = 1/(ncol(train_nolog)-1), cost = 1)

yhat_svm1 <- predict(svm_base, newdata = test_nolog)
perr_svm1 <- sum(as.numeric(yhat_svm1)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm1<- table(ifelse(as.numeric(yhat_svm1)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm1 <- prediction(as.numeric(yhat_svm1)-1, test_nolog$mt50K)
auc_svm1 <- performance(rocr_obj_svm1, "auc")@y.values[[1]]
```

#### Primary model

The parameters of the primary model were:

* radial kernel 
* gamma = 1 / 13 ~ 0.07692308
* cost=1 

This resulted `r perr_svm1*100`% overall prediction error and `r auc_svm1` AUC.

#### Grid search

```{r svm_grid_srch2, echo=FALSE,  warning=FALSE}

svm_2 <- svm(mt50K ~ ., data = train_nolog,
                kernel = "radial", gamma = 2^-7, cost = 2)
yhat_svm2 <- predict(svm_2, newdata = test_nolog)
perr_svm2 <-sum(as.numeric(yhat_svm2)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm2<- table(ifelse(as.numeric(yhat_svm2)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm2 <- prediction(as.numeric(yhat_svm2)-1, test_nolog$mt50K)
auc_svm2 <- performance(rocr_obj_svm2, "auc")@y.values[[1]]

```

```{r svm_grid_srch3, echo=FALSE,  warning=FALSE}
svm_3 <- svm(mt50K ~ ., data = train_nolog,
             kernel = "radial", gamma = 2^-3, cost = 2)
yhat_svm3 <- predict(svm_3, newdata = test_nolog)
perr_svm3 <- sum(as.numeric(yhat_svm3)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm3<- table(ifelse(as.numeric(yhat_svm3)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm3 <- prediction(as.numeric(yhat_svm3)-1, test_nolog$mt50K)
auc_svm3 <- performance(rocr_obj_svm3, "auc")@y.values[[1]]

```

```{r svm_grid_srch4, echo=FALSE,  warning=FALSE}
svm_4 <- svm(mt50K ~ ., data = train_nolog,
             kernel = "radial", gamma = 2^-1, cost = 2)
yhat_svm4 <- predict(svm_4, newdata = test_nolog)
perr_svm4 <- sum(as.numeric(yhat_svm4)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm4<- table(ifelse(as.numeric(yhat_svm4)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm4 <- prediction(as.numeric(yhat_svm4)-1, test_nolog$mt50K)
auc_svm4 <- performance(rocr_obj_svm4, "auc")@y.values[[1]]
```

```{r svm_grid_srch5, echo=FALSE,  warning=FALSE}
svm_5 <- svm(mt50K ~ ., data = train_nolog,
             kernel = "radial", gamma = 2^-3, cost = 1)
yhat_svm5 <- predict(svm_5, newdata = test_nolog)
perr_svm5 <- sum(as.numeric(yhat_svm5)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm5<- table(ifelse(as.numeric(yhat_svm5)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm5 <- prediction(as.numeric(yhat_svm5)-1, test_nolog$mt50K)
auc_svm5 <- performance(rocr_obj_svm5, "auc")@y.values[[1]]
```

```{r svm_grid_srch6, echo=FALSE,  warning=FALSE}
svm_6 <- svm(mt50K ~ ., data = train_nolog,
             kernel = "radial", gamma = 2^-3, epsilon = 0.2)
yhat_svm6 <- predict(svm_6, newdata = test_nolog)
perr_svm6 <- sum(as.numeric(yhat_svm6)-1!=test_nolog$mt50K)/nrow(test_nolog)
tsvm6<- table(ifelse(as.numeric(yhat_svm6)-1>0,1,0), test_nolog$mt50K)
rocr_obj_svm6 <- prediction(as.numeric(yhat_svm6)-1, test_nolog$mt50K)
auc_svm6 <- performance(rocr_obj_svm6, "auc")@y.values[[1]]
```

|ModelID |kernel|Gamma|cost|epsilon|error rate|Result TN|Result FN|Result FP|Result TP|AUC|
|:------:|:----:|:---:|:--:|:-----:|:--------:|:-------:|:-------:|:-------:|:-------:|:-:|
|base smv|radial|1/13=0.07692308|1|default=0.1|`r perr_svm1`|`r tsvm1[1,1]`|`r tsvm1[1,2]`|`r tsvm1[2,1]`|`r tsvm1[2,2]`|`r auc_svm1`|
|smv2|radial|2^-7=0.0078125|2|default=0.1|`r perr_svm2`|`r tsvm2[1,1]`|`r tsvm2[1,2]`|`r tsvm2[2,1]`|`r tsvm2[2,2]`|`r auc_svm2`|
|smv3|radial|2^-3=0.125|2|default=0.1|`r perr_svm3`|`r tsvm3[1,1]`|`r tsvm3[1,2]`|`r tsvm3[2,1]`|`r tsvm3[2,2]`|`r auc_svm3`|
|smv4|radial|2^-1=0.5|2|default=0.1|`r perr_svm4`|`r tsvm4[1,1]`|`r tsvm4[1,2]`|`r tsvm4[2,1]`|`r tsvm4[2,2]`|`r auc_svm4`|
|smv5|radial|2^-3=0.125|1|default=0.1|`r perr_svm5`|`r tsvm5[1,1]`|`r tsvm5[1,2]`|`r tsvm5[2,1]`|`r tsvm5[2,2]`|`r auc_svm5`|
|smv5|radial|2^-3=0.125|default=1|0.2|`r perr_svm6`|`r tsvm6[1,1]`|`r tsvm6[1,2]`|`r tsvm6[2,1]`|`r tsvm6[2,2]`|`r auc_svm6`|

## Model selection

* All 3 models have produced relatively good resutls during the modeling on this classification use-case on this dataset
* GBM with appropriate parametering has performed a little better in terms of error rate
* My GBM and Random forest models have produced more false positives
* SVM had a very narrow range of AUC results, lower AUC than GBM, but still comparable to random forest
* The parametering of GBM and Random Forest are more intuitve than SVM, SVM reveals the very high mathematical complexity the modeller ought to know - without providing significantly higher payback in performance.
* The model selection would depend on the use case, specifically
  + The expected true positive/false positive ratio
  + The cost of false positive vs false negative 
  + The environment where the prediction needs to be executed
  
|Best model of genere with parameters       |Error rate/AUC                         |FN/FP ratio            |TP/FP ratio             |
|:-----------------------------------------:|:---------------------------------:|:---------------------:|:----------------------:|
|RF, ntree=500, cap.log, variable=3         |AUC =`r auc3` |`r rft3[1,2]/rft3[2,1]`|`r rft3[2,2]/rft3[2,1]` |
|GBM, ntree=500, int.depth=20, shrinkage=0.01, cv =5fold|AUC= `r auc_gbm4`|`r t4[1,2]/t4[2,1]`|`r t4[2,2]/t4[2,1]` |
|SVM2, kernel=radial, gamma=1/2^7, cost=2, epsilon=0.1|AUC= `r auc_svm2`|`r tsvm2[1,2]/ tsvm2[2,1]`|`r tsvm2[2,2]/ tsvm2[2,1]`||


## Discussion of effectiveness

* Random forest was a bit inaccurate, but was able to run on the dataset without issues (note: no factors have exceeded 53, which is RF limitation)
* GBM was the most accurate, but the cost was more feature engineering and in some cases model was not able to predict positive outcome
* SVM has very robust results, but the accuracy is lower, than GBM
* The price SVM and Random Forest pays in accuracy pays back in ranfom rorest inability to overfit and SVM also being a robust, not overfitting model.


## Appendix

### Notes about grid search based SVM tuning

I used manual grid search for all models instead of tune.svm or tune.randomForest. I gave an attempt to these software aider more detailed ones, but the modeling of the 5x5 or 7x7 models in case of SVM took such a long time (4+ hours) that I aborted them. I clearly reached a limitation of my local computer (Intel Core i5-6200U CPU @ 2.3GHz). This would have needed running on a stronger server.

### Notes about the performance

There is a possibility, that I could have achieved better performance model with ensembles. This however in time shortage I have not given an attempt in the scope of this project.

### Analysis with KNN 

K-NearestNeighbour classification algorythm is a bit different from the tree models above, because it can handle only numeric input vectors. It uses Eucledian distance measure thereafter to calculate the "distance of the neighborhood".
From the other side it uses a factor for classification.

```{r knn_setup, echo=FALSE, warning=FALSE}

library(class)
train_nolog$mt50K <- as.factor(train_nolog$mt50K)
test_nolog$mt50K <- as.factor(test_nolog$mt50K)

cdf2 <- cdf

cdf2$work_class <- as.numeric(cdf2$work_class)
cdf2$education <- as.numeric(cdf2$education )
cdf2$mar_stat <- as.numeric(cdf2$mar_stat)
cdf2$occupation <- as.numeric(cdf2$occupation)
cdf2$relationship <- as.numeric(cdf2$relationship)
cdf2$race <- as.numeric(cdf2$race)
cdf2$sex <- as.numeric(cdf2$sex)
cdf2$nat_ctry <- as.numeric(cdf2$nat_ctry)
cdf2$rnd <- NULL

train_nolog2 <- cdf2[0:round( nrow(cdf2) * 0.7 ),]
test_nolog2 <- cdf2[(round( nrow(cdf2) * 0.7 )+1) : nrow(cdf2),]
```


### 2-NN

The following table shows the confusion matrix, using the same (but transformed) train and test sets, using the 2 nearest neighbors for prediction.

```{r knn_2nn, echo=FALSE, warning=FALSE}
fit2 <- knn(train_nolog2[,1:13], test_nolog2[,1:13], train_nolog2$mt50K, k = 2, prob=TRUE)
t2nn <- table(test_nolog2$mt50K,fit2)
pander(t2nn)
```

### 7-NN

The following table shows the confusion matrix, using the same (but transformed) train and test sets, using the 7 nearest neighbors for prediction.

```{r knn_7nn, echo=FALSE, warning=FALSE}
fit7 <- knn(train_nolog2[,1:13], test_nolog2[,1:13], train_nolog2$mt50K, k = 7, prob=TRUE)
t7nn <- table(test_nolog2$mt50K,fit7)
pander(t7nn)
```


### 13-NN

The following table shows the confusion matrix, using the same (but transformed) train and test sets, using the 13 nearest neighbors for prediction.


```{r knn_13nn, echo=FALSE, warning=FALSE}
fit13 <- knn(train_nolog2[,1:13], test_nolog2[,1:13], train_nolog2$mt50K, k = 13, prob=TRUE)
t13nn <- table(test_nolog2$mt50K,fit13)
pander(t13nn)

```

### Conclusion of KNN modeling

The following chart shows the results of the models. 2 neighbours show a higher error rate, however there is small difference between 7 and 13 neighbors. On this specific database KNN algorythm has produced surprisingly good results, despite of the fact that there was no real feature engineering and analysis how the distances between factor levels translate to Eucledian distance.

 For a model which gives class as output, will be represented as a single point in ROC plot.

|Number of neighbors|Result TN     |Result FN     |Result FP    |Result TP    |Error rate                                    |
|:-----------------:|:------------:|:------------:|:-----------:|:-----------:|:----------------------------------------------:|
|2                  |`r t2nn[1,1]` |`r t2nn[1,1]` |`r t2nn[1,1]`|`r t2nn[1,1]`|`r (t7nn[1,2]+t7nn[2,1])/nrow(test_nolog)*100`% |
|7                  |`r t7nn[1,1]` |`r t7nn[1,1]` |`r t7nn[1,1]`|`r t7nn[1,1]`|`r (t7nn[1,2]+t7nn[2,1])/nrow(test_nolog)*100`% |
|13                 |`r t13nn[1,1]`|`r t13nn[1,1]`|`r t13nn[1,1]`|`r t13nn[1,1]`|`r (t13nn[1,2]+t13nn[2,1])/nrow(test_nolog)*100`% |

### Reason of exluding from modeling

I decided excluding KNN from modeling because I was not able to calculate AUC for some technical reasons. KNN produces 





